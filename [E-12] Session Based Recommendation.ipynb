{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch==1.10.0+cpu torchvision==0.11.1+cpu torchaudio==0.10.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-clark",
   "metadata": {},
   "source": [
    "# SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-medline",
   "metadata": {},
   "source": [
    "https://arxiv.org/pdf/1511.06939v4.pdf\n",
    "\n",
    "Abstract  \n",
    ": 우리는 새로운 영역, 즉 추천 시스템에 순환 신경망(RNN)을 적용합니다. 실제 추천 시스템은 긴 사용자 이력(예: Netflix 시청기록) 대신 짧은 세션 기반 데이터(예: 작은 스포츠웨어 웹사이트 세션 기록)에 추천해야 하는 경우가 많습니다. 이 경우, 행렬 분해 접근 방식은 정확하지 않습니다. 이러한 문제는 일반적으로 항목간 추천(item-to-item), 즉 유사한 항목을 추천함으로써 해결됩니다. 우리는 전체 세션을 모델링함으로써 더 정확한 추천을 제공할 수 있다고 생각합니다. 따라서 세션 기반 권장 사항에 대한 RNN 기반 접근 방식을 제안합니다. 우리는 또한 task에 효율적인(practical) 방법을 도입하며, 위 문제를 더 유용하게 해결할 수 있는 ranking loss 함수와 같은 개선사항을 클래식 RNN에 적용합니다.\n",
    " 두 데이터 세트에 대한 실험 결과는 기존에 사용되던 접근 방식에 비해 현저한 개선을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-intelligence",
   "metadata": {},
   "source": [
    "1. Introduction  \n",
    ": 생략\n",
    "2. Related work  \n",
    ": 생략\n",
    "3. RECOMMENDATIONS WITH RNNS  \n",
    ": 순환 신경망은 가변 길이 시퀀스 데이터를 모델링하기 위해 고안되었습니다. RNN과 기존 Feed-Forward 모델의 주요 차이점은 네트워크를 구성하는 단위에 hidden state 상태가 존재한다는 것입니다. 기본 RNN은 다음 업데이트 기능을 사용하여 숨겨진 상태 h를 업데이트합니다.\n",
    "$$\n",
    "h_t = g(Wx_t + Uh_{t-1})\n",
    "$$\n",
    "여기서 g는 로지스틱 시그모이드 함수와 같은 smooth and bounded 함수입니다. $x_t$s는 단위 시간 t에서의 입력입니다.  RNN은 현재 상태 $h_t$가 주어지면 시퀀스의 다음 요소에 대한 확률 분포를 출력합니다.  \n",
    "GRU(Cho et al., 2014)는 기울기 소실 문제를 해결하는 것을 목표로 하는 기본 RNN보다 정교한 모델입니다. GRU 게이트는 기본적으로 유닛의 hidden state를 업데이트할 시기와 양을 학습합니다. GRU의 활성화는 이전 활성화와 후보 활성화 $\\hat{h_t}$ 사이의 선형 보간입니다.\n",
    "$$\n",
    "h_t = (1-z_t)h_{t-1}+z_t\\hat{h_t}\n",
    "$$\n",
    "where the update gate is given by:\n",
    "$$\n",
    "z_t = \\sigma{(W_zx_t + U_zh_{t-1})}\n",
    "$$\n",
    "while the candidate activation function $\\hat{h_t}$ is computed in a similar manner:\n",
    "$$\n",
    "\\hat{h_t} = tanh(Wx_t + U(r_t \\odot h_{t-1})\n",
    "$$\n",
    "and finaly the reset gate $r_t$ is given by:\n",
    "$$\n",
    "r_t = \\sigma(W_rx_t + U_rh_{t-1})\n",
    "$$\n",
    "\n",
    "3.1 Customizing the GRU model\n",
    "<img src=https://i.ibb.co/RPVM70D/2021-11-04-01-21-30.png width=600></img>\n",
    ": 세션 기반 추천 모델에 GRU 기반 RNN을 사용했습니다. 네트워크의 입력은 세션의 actual state이고 출력은 세션의 다음 이벤트 항목입니다. 세션의 상태는 실제 이벤트의 item이거나 지금까지 세션의 이벤트일 수 있습니다. 전자의 경우 1-of-N 인코딩이 사용됩니다. (즉, 입력 벡터의 길이는 item 수와 같고 활성 항목에 해당하는 좌표만 1이고 나머지는 0입니다.)  \n",
    "후자는 더 일찍 발생한 이벤트에 discount되는 representations의 가중 합계를 사용합니다. 안정성을 위해 입력 벡터가 정규화됩니다. 우리는 이것이 메모리 효과를 향상시킬 것으로 기대합니다. RNN의 더 긴 메모리에 의해 잘 포착되지 않는 매우 국소적인 순서 제약의 강화입니다. 우리는 또한 추가 임베딩 레이어를 추가하는 실험을 했지만 1-of-N 인코딩이 항상 더 나은 성능을 보였습니다.  \n",
    "네트워크의 핵심은 GRU 계층이며 마지막 계층과 출력 사이에 추가 피드포워드 계층을 추가할 수 있습니다. 출력은 항목의 예측된 선호도, 즉 각 항목에 대한 세션의 다음 항목이 될 가능성입니다. 여러 GRU 레이어를 사용하는 경우 이전 레이어의 은닉 상태가 다음 레이어의 입력이 됩니다. 입력은 네트워크의 더 깊은 GRU 레이어에 선택적으로 연결할 수도 있습니다. 이것이 성능을 향상시킨다는 것을 발견했기 때문입니다. 이벤트의 시계열 내에서 단일 이벤트의 표현을 묘사하는 그림 1의 전체 아키텍처를 참조하십시오.  \n",
    "추천 시스템은 순환 신경망의 주요 응용 분야가 아니므로 작업에 더 적합하도록 기본 네트워크를 수정했습니다.  우리의 솔루션이 실제 환경에 적용될 수 있도록 실용적인 점도 고려했습니다.\n",
    "  \n",
    "3.1.1 Session-Parallel Mini-Baches\n",
    "<img src=https://i.ibb.co/1bwBcKB/2021-11-04-01-21-37.png width=400></img>\n",
    "자연어 처리 작업을 위한 RNN은 일반적으로 순차 미니 배치를 사용합니다.  예를 들어 문장의 단어 위에 슬라이딩 창을 사용하고 이러한 창 조각을 서로 옆에 놓아 미니 배치를 형성하는 것이 일반적입니다.  이것은 우리의 작업에 맞지 않습니다. 왜냐하면 (1) 세션의 길이 분포가 문장의 경우보다 훨씬 더 많이 다를 수 있습니다. 일부 세션은 단 2개의 이벤트로 구성되는 반면 다른 세션은 수백 개가 넘는 이벤트로 구성될 수 있습니다. (2) 우리의 목표는 세션이 시간이 지남에 따라 어떻게 발전하는지 캡처하는 것이므로 조각으로 나누는 것은 의미가 없습니다.  따라서 세션 병렬 미니 배치를 사용합니다.  먼저 세션에 대한 순서를 만듭니다.  그런 다음 첫 번째 X 세션의 첫 번째 이벤트를 사용하여 첫 번째 미니 배치의 입력을 형성합니다(원하는 출력은 활성 세션의 두 번째 이벤트입니다).  두 번째 미니 배치는 두 번째 이벤트 등으로 구성됩니다.  세션 중 하나가 종료되면 사용 가능한 다음 세션이 그 자리에 놓입니다.  세션은 독립적인 것으로 간주되므로 이 전환이 발생할 때 적절한 숨김 상태를 재설정합니다.  자세한 내용은 그림 2를 참조하십시오.\n",
    "\n",
    "  \n",
    "3.1.2 Sampling on the output\n",
    "추천 시스템은 항목 수가 많을 때 특히 유용합니다. 중간 규모의 인터넷 쇼핑몰의 경우에도 수만개의 종류가 있지만 더 큰 사이트는 수십만 개 또는 수백만 개의 항목이 있는 경우가 많습니다. 각 단계에서 각 항목에 대한 점수를 계산하면 알고리즘이 항목 수와 이벤트 수의 곱으로 확장됩니다. 이것은 너무 커 실제로 사용할 수 없습니다. 따라서 출력을 샘플링하고 항목의 작은 하위 집합에 대한 점수만 계산해야 합니다. 이것은 또한 일부 가중치만 업데이트됨을 의미합니다. 원하는 출력 외에도 일부 부정적인 examples에 대한 점수를 계산하고 원하는 출력이 높은 순위가 되도록 가중치를 수정해야 합니다.  \n",
    "arbitrary(임의) missing 이벤트의 자연스러운 해석(natural interpretation)은 사용자가 해당 항목의 존재를 알지 못하므로 상호 작용이 없었다고 보는 것입니다. 그러나 사용자가 항목에 대해 알고 있었고 항목을 싫어했기 때문에 상호 작용하지 않기로 선택했을 가능성은 낮습니다. 아이템의 인기가 높을수록 사용자가 알고 있을 가능성이 높기 때문에 누락된 이벤트가 싫어함을 표시할 가능성이 높습니다. 따라서 인기도에 비례하여 항목을 샘플링해야 합니다. 각 훈련 예제에 대해 별도의 샘플을 생성하는 대신 미니 배치의 다른 훈련 예제의 항목을 부정적인 예제로 사용합니다. 이 접근 방식의 이점은 샘플링을 건너뛰어 계산 시간을 더욱 줄일 수 있다는 것입니다. 또한 코드를 덜 복잡하게 만들어 더 빠른 매트릭스 작업으로 구현하는 측면에서도 이점이 있습니다. 한편, 이 접근 방식은 인기도 기반 샘플링이기도 합니다. 항목이 미니 배치의 다른 교육 예제에 포함될 가능성은 인기도에 비례하기 때문입니다. \n",
    "\n",
    "3.1.3 Ranking Loss\n",
    "\n",
    "- BPR(Bayesian Personalized Ranking):\n",
    "$$\n",
    "L_s = -\\frac{1}{N_S} \\cdot \\sum^{N_S}_{j=1} log(\\sigma(\\hat{r}_{s,i} - \\hat{r}_{s,j}))\n",
    "$$\n",
    "\n",
    "- TOP1: \n",
    "$$\n",
    "L_s = \\frac{1}{N_S} \\cdot \\sum^{N_S}_{j=1} \\sigma(\\hat{r}_{s,j} - \\hat{r}_{s,i}) + \\sigma(\\hat{r}^2_{s,j})\n",
    "$$\n",
    "  \n",
    "4. EXPERIMENTS  \n",
    ": \n",
    "2가지 데이터셋을 이용했고, 첫번째 데이터셋은 RecSys Challenge 2015. 2번째 데이터셋은 OTT video service platform. 첫번째 데이터셋의 경우, 길이 1 짜리 세션은 train, test set 모두 삭제했고, train set에 있지만 test set에 없는 아이템의 경우도 삭제했습니다. \n",
    "<img src=https://i.ibb.co/x36sDzN/2021-11-04-01-22-00.png width=400></img>\n",
    "<img src=https://i.ibb.co/JQg5TZM/2021-11-04-01-22-04.png width=400></img>\n",
    "\n",
    "5. Conclusion & Future work  \n",
    ":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-negative",
   "metadata": {},
   "source": [
    "https://github.com/hungthanhpham94/GRU4REC-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-saint",
   "metadata": {},
   "source": [
    "https://github.com/yhs968/pyGRU4REC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-factory",
   "metadata": {},
   "source": [
    "# Data 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('events.csv')\n",
    "df = df.sort_values(['visitorid', 'timestamp']).reset_index(drop=True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "df['date'] = df['timestamp'].dt.strftime('%Y%m%d').astype(int)\n",
    "df = df.drop(['transactionid'], axis=1)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 5% 정도의 크기를 val, test set으로 선정\n",
    "train = df[df['date'] < 20150816]\n",
    "val = df[(df['date'] >= 20150816) & (df['date'] < 20150819)]\n",
    "test = df[df['date'] >= 20150819]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_session(df, session_id, way='short', size=200):\n",
    "    session_size = df.groupby(session_id).size()\n",
    "    if way == 'short':\n",
    "        df = df[np.in1d(df[session_id], session_size[session_size > 1].index)]\n",
    "    else:\n",
    "        df = df[np.in1d(df[session_id], session_size[session_size < size].index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = remove_session(train, 'visitorid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-charm",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('visitorid').size().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('visitorid').size()[(train.groupby('visitorid').size() > 200) & (train.groupby('visitorid').size() < 500)].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('visitorid').size()[(train.groupby('visitorid').size() > 100) & (train.groupby('visitorid').size() < 200)].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.groupby('visitorid').size()[(train.groupby('visitorid').size() > 50) & (train.groupby('visitorid').size() < 100)].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = remove_session(train, 'visitorid', 'long')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논문에서 제시한 방법대로 train에 있지만 test에 없는 item은 삭제한다.\n",
    "val = val[np.in1d(val['itemid'], train['itemid'])]\n",
    "test = test[np.in1d(test['itemid'], train['itemid'])]\n",
    "\n",
    "# 논문에서 제시한 방법대로 세션 길이가 1인 항목은 삭제한다.\n",
    "val = remove_session(val, 'visitorid')\n",
    "test = remove_session(test, 'visitorid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-underwear",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train Set has', len(train), 'Events, ', train['visitorid'].nunique(), 'Sessions, and', train['itemid'].nunique(), 'Items\\n')\n",
    "print('Val Set has', len(val), 'Events, ', val['visitorid'].nunique(), 'Sessions, and', val['itemid'].nunique(), 'Items\\n')\n",
    "print('Test Set has', len(test), 'Events, ', test['visitorid'].nunique(), 'Sessions, and', test['itemid'].nunique(), 'Items\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-thickness",
   "metadata": {},
   "source": [
    "# Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, session_id='visitorid', item_id='itemid', timestamp='timestamp'):\n",
    "        self.df = df\n",
    "        self.session_id = session_id\n",
    "        self.item_id = item_id\n",
    "        self.timestamp = timestamp\n",
    "        self.add_item_idx()\n",
    "        self.df = self.df.sort_values([self.session_id, self.timestamp])\n",
    "        self.offsets = self.get_offset()\n",
    "        self.session_idx = np.arange(self.df[self.session_id].nunique())\n",
    "        self.items = self.item_map[self.item_id].unique()\n",
    "        \n",
    "    def get_offset(self):\n",
    "        # session id마다 길이를 구하고, 해당 값을 누적합.\n",
    "        # ex. 1, 1, 2, 2, 2, 3, 4,\n",
    "        # => 0, 2, 3, 1, 1, \n",
    "        # => 0, 2, 5, 6, 7\n",
    "        offsets = np.zeros(self.df[self.session_id].nunique() + 1, dtype=np.int32)\n",
    "        offsets[1:] = self.df.groupby(self.session_id).size().cumsum()\n",
    "        return offsets\n",
    "    \n",
    "    def add_item_idx(self):\n",
    "        unique_items = self.df[self.item_id].unique()\n",
    "        self.item_map = pd.DataFrame({'itemid': unique_items, 'item_idx': range(len(unique_items))})\n",
    "        self.df = pd.merge(self.df, self.item_map, on=self.item_id, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=10):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset.df = dataset.df\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        df = self.dataset.df\n",
    "        offsets = self.dataset.offsets\n",
    "        session_idx = self.dataset.session_idx\n",
    "\n",
    "        iters = np.arange(self.batch_size)\n",
    "        max_iter = iters.max()\n",
    "        start, end = offsets[session_idx[iters]], offsets[session_idx[iters] + 1]\n",
    "        mask = []\n",
    "        finished = False\n",
    "\n",
    "        while not finished:\n",
    "            minlen = (end - start).min()\n",
    "\n",
    "            for i in range(minlen - 1):\n",
    "                idx_input = df['item_idx'].values[start]\n",
    "                idx_target = df['item_idx'].values[start + i + 1]\n",
    "                input_ = torch.LongTensor(idx_input)\n",
    "                target = torch.LongTensor(idx_target)\n",
    "                yield input_, target, mask\n",
    "\n",
    "            start = start + (minlen - 1)\n",
    "            mask = np.arange(len(iters))[(end - start) <= 1]\n",
    "            for idx in mask:\n",
    "                max_iter += 1\n",
    "                if max_iter >= len(offsets) - 1:\n",
    "                    finished = True\n",
    "                    break\n",
    "                iters[idx] = max_iter\n",
    "                start[idx] = offsets[session_idx[max_iter]]\n",
    "                end[idx] = offsets[session_idx[max_iter] + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train)\n",
    "train_loader = DataLoader(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(val)\n",
    "val_loader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-right",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels, mask =  next(iterator)\n",
    "print(f'Model Input Item Idx are : {inputs}')\n",
    "print(f'Label Item Idx are : {\"\":5} {labels}')\n",
    "print(f'Previous Masked Input Idx are {mask}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-thousand",
   "metadata": {},
   "source": [
    "# Loss 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-theorem",
   "metadata": {},
   "source": [
    "## BPR Loss 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = torch.tensor([\n",
    "    [1., 2., 3.],\n",
    "    [4., 5., 6.],\n",
    "    [7., 8., 9.]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.diag().view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.diag().view(-1, 1).expand_as(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "logit.diag().view(-1, 1).expand_as(logit) - logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.logsigmoid(logit.diag().view(-1, 1).expand_as(logit) - logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -torch.mean(F.logsigmoid(logit.diag().view(-1, 1).expand_as(logit) - logit))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flying-chancellor",
   "metadata": {},
   "source": [
    "## TOP 1 Loss 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(diff).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sigmoid(diff ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-saskatchewan",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = F.sigmoid(diff).mean() + F.sigmoid(diff ** 2).mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BPRLoss(logit):\n",
    "    diff = logit.diag().view(-1, 1).expand_as(logit) - logit\n",
    "    loss = -torch.mean(F.logsigmoid(diff))\n",
    "\n",
    "    return loss\n",
    "    \n",
    "\n",
    "    \n",
    "def TOP1Loss(logit):\n",
    "    diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)\n",
    "    loss = F.sigmoid(diff).mean() + F.sigmoid(logit ** 2).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-tennessee",
   "metadata": {},
   "source": [
    "# Metric 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall(indices, targets):\n",
    "    targets = targets.view(-1, 1).expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    if len(hits) == 0:\n",
    "        return 0\n",
    "    n_hits = (targets == indices).nonzero()[:, :-1].size(0)\n",
    "    recall = float(n_hits) / targets.size(0)\n",
    "    return recall\n",
    "\n",
    "\n",
    "def get_mrr(indices, targets):\n",
    "    tmp = targets.view(-1, 1)\n",
    "    targets = tmp.expand_as(indices)\n",
    "    hits = (targets == indices).nonzero()\n",
    "    ranks = hits[:, -1] + 1\n",
    "    ranks = ranks.float()\n",
    "    rranks = torch.reciprocal(ranks)\n",
    "    mrr = torch.sum(rranks).data / targets.size(0)\n",
    "    return mrr\n",
    "\n",
    "\n",
    "def evaluate(indices, targets, k=20):\n",
    "    _, indices = torch.topk(indices, k, -1)\n",
    "    recall = get_recall(indices, targets)\n",
    "    mrr = get_mrr(indices, targets)\n",
    "    return recall, mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-recognition",
   "metadata": {},
   "source": [
    "# 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class GRU4REC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1,\n",
    "                 dropout_hidden=.5, dropout_input=0, batch_size=50, use_cuda=False):\n",
    "        super(GRU4REC, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_hidden = dropout_hidden\n",
    "        self.dropout_input = dropout_input\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.onehot_buffer = self.init_emb()\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "        self.final_activation = nn.Tanh()\n",
    "        self.gru = nn.GRU(self.input_size, self.hidden_size, self.num_layers, dropout=self.dropout_hidden)\n",
    "        self = self.to(self.device)\n",
    "\n",
    "\n",
    "    def forward(self, inp, hidden):\n",
    "\n",
    "        embedded = self.onehot_encode(inp)\n",
    "        if self.training and self.dropout_input > 0:\n",
    "            embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.unsqueeze(0)\n",
    "\n",
    "        output, hidden = self.gru(embedded, hidden) #(num_layer, B, H)\n",
    "        output = output.view(-1, output.size(-1))  #(B,H)\n",
    "        logit = self.final_activation(self.h2o(output))\n",
    "\n",
    "        return logit, hidden\n",
    "\n",
    "    def init_emb(self):\n",
    "        onehot_buffer = torch.FloatTensor(self.batch_size, self.output_size)\n",
    "        onehot_buffer = onehot_buffer.to(self.device)\n",
    "        return onehot_buffer\n",
    "\n",
    "    def onehot_encode(self, inp):\n",
    "        self.onehot_buffer.zero_()\n",
    "        index = inp.view(-1, 1)\n",
    "        one_hot = self.onehot_buffer.scatter_(1, index, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def embedding_dropout(self, inp):\n",
    "        p_drop = torch.Tensor(inp.size(0), 1).fill_(1 - self.dropout_input)\n",
    "        mask = torch.bernoulli(p_drop).expand_as(inp) / (1 - self.dropout_input)\n",
    "        mask = mask.to(self.device)\n",
    "        inp = inp * mask\n",
    "        return inp\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.num_layers, self.batch_size, self.hidden_size).to(self.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technical-dodge",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, params, lr=.05, momentum=0, weight_decay=0, eps=1e-6):\n",
    "        self.optimizer = optim.Adagrad(params, lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sophisticated-production",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Evaluation:\n",
    "    def __init__(self, model, loss_func, use_cuda, k=20):\n",
    "        self.model = model\n",
    "        self.loss_func = loss_func\n",
    "        self.topk = k\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "    def eval(self, eval_data, batch_size):\n",
    "        self.model.eval()\n",
    "        losses = []\n",
    "        recalls = []\n",
    "        mrrs = []\n",
    "        dataloader = DataLoader(eval_data, batch_size)\n",
    "        with torch.no_grad():\n",
    "            hidden = self.model.init_hidden()\n",
    "            for ii, (input, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "                input = input.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "                logit, hidden = self.model(input, hidden)\n",
    "                logit_sampled = logit[:, target.view(-1)]\n",
    "                loss = self.loss_func(logit_sampled)\n",
    "                recall, mrr = evaluate(logit, target, k=self.topk)\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                recalls.append(recall)\n",
    "                mrrs.append(mrr)\n",
    "        mean_losses = np.mean(losses)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_mrr = np.mean(mrrs)\n",
    "\n",
    "        return mean_losses, mean_recall, mean_mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_data, eval_data, optim, use_cuda, loss_func, batch_size):\n",
    "        self.model = model\n",
    "        self.train_data = train_data\n",
    "        self.eval_data = eval_data\n",
    "        self.optim = optim\n",
    "        self.loss_func = loss_func\n",
    "        self.evaluation = Evaluation(self.model, self.loss_func, use_cuda, k=20)\n",
    "        self.device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, end_epoch, start_time=None):\n",
    "        if start_time is None:\n",
    "            self.start_time = time.time()\n",
    "        else:\n",
    "            self.start_time = start_time\n",
    "\n",
    "        for epoch in range(1, end_epoch+1):\n",
    "            st = time.time()\n",
    "            print('Start Epoch #', epoch)\n",
    "            train_loss = self.train_epoch(epoch)\n",
    "            loss, recall, mrr = self.evaluation.eval(self.eval_data, self.batch_size)\n",
    "            print(\"Epoch: {}, train loss: {:.4f}, loss: {:.4f}, recall: {:.4f}, mrr: {:.4f}, time: {}\".format(epoch, train_loss, loss, recall, mrr, time.time() - st))\n",
    "            checkpoint = {\n",
    "                'model': self.model,\n",
    "                'epoch': epoch,\n",
    "                'optim': self.optim,\n",
    "                'loss': loss,\n",
    "                'recall': recall,\n",
    "                'mrr': mrr\n",
    "            }\n",
    "            model_name = f\"model_epoch{epoch}.pt\"\n",
    "            torch.save(checkpoint, model_name)\n",
    "            print(\"Save model as %s\" % model_name)\n",
    "\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        losses = []\n",
    "\n",
    "        def reset_hidden(hidden, mask):\n",
    "            if len(mask) != 0:\n",
    "                hidden[:, mask, :] = 0\n",
    "            return hidden\n",
    "\n",
    "        hidden = self.model.init_hidden()\n",
    "        dataloader = DataLoader(self.train_data, self.batch_size)\n",
    "        for ii, (input, target, mask) in tqdm(enumerate(dataloader), total=len(dataloader.dataset.df) // dataloader.batch_size, miniters = 1000):\n",
    "            input = input.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            self.optim.zero_grad()\n",
    "            hidden = reset_hidden(hidden, mask).detach()\n",
    "            logit, hidden = self.model(input, hidden)\n",
    "            # output sampling\n",
    "            logit_sampled = logit[:, target.view(-1)]\n",
    "            loss = self.loss_func(logit_sampled)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "        mean_losses = np.mean(losses)\n",
    "        return mean_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(train_dataset.items)\n",
    "hidden_size = 100\n",
    "output_size = input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU4REC(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataset, val_dataset, optimizer, False, BPRLoss, 50)\n",
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-arcade",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('model_epoch1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emotional-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = checkpoint[\"model\"]\n",
    "loaded_model.gru.flatten_parameters()\n",
    "evaluation = Evaluation(loaded_model, BPRLoss, use_cuda=False, k=20)\n",
    "loss, recall, mrr = evaluation.eval(val_dataset, 50)\n",
    "print(f\"Final result: recall = {round(recall, 2)}, mrr = {round(mrr, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
