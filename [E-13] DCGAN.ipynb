{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quality-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-horizontal",
   "metadata": {},
   "source": [
    "STEP 1. 데이터셋 구성하기\n",
    "- 학습에 사용할 train_x의 이미지를 -1, 1로 정규화합니다.\n",
    "- 로드한 학습 데이터를 시각화를 통해 확인해 봅시다.\n",
    "- tf.data.Dataset 모듈의 from_tensor_slices() 함수를 사용하여 미니배치 데이터셋을 구성해 봅시다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "rural-accordance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10 = tf.keras.datasets.cifar10\n",
    "\n",
    "(train_x, _), (test_x, _) = cifar10.load_data()\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "available-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(train_x[i])\n",
    "    plt.title(f'index: {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-coach",
   "metadata": {},
   "source": [
    "<img src=https://user-images.githubusercontent.com/43724189/140882377-67e87c99-d8bc-4e17-b699-6690d3871380.png width=400></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "legal-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = (train_x - 127.5) / 127.5\n",
    "test_x = (test_x - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applied-vitamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "naughty-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_x).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "amended-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Dense layer\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Second: Reshape layer\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "\n",
    "    # Third: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=(5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fourth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Fifth: Conv2DTranspose layer\n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=(5, 5), strides=(2, 2), padding='same', use_bias=False, \\\n",
    "                                     activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "capable-labor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16384)             1638400   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 16384)             65536     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 8, 8, 128)         819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 16, 16, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 32, 32, 3)         4800      \n",
      "=================================================================\n",
      "Total params: 2,733,504\n",
      "Trainable params: 2,700,352\n",
      "Non-trainable params: 33,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pleasant-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal([1, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structural-samba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32, 32, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_image = generator(noise, training=False)\n",
    "generated_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continent-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "\n",
    "    # Start\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # First: Conv2D Layer\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[32, 32, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Second: Conv2D Layer\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    # Third: Flatten Layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fourth: Dense Layer\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "assisted-findings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 64)        4864      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 217,985\n",
      "Trainable params: 217,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "known-convention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.00632527]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision = discriminator(generated_image, training=False)\n",
    "decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "established-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "innovative-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "considered-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "external-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_accuracy(real_output, fake_output):\n",
    "    real_accuracy = tf.reduce_mean(tf.cast(tf.math.greater_equal(real_output, tf.constant([0.5])), tf.float32))\n",
    "    fake_accuracy = tf.reduce_mean(tf.cast(tf.math.less(fake_output, tf.constant([0.5])), tf.float32))\n",
    "    return real_accuracy, fake_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "characteristic-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "racial-reach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([16, 100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "seed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sunset-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):  #(1) 입력데이터\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])  #(2) 생성자 입력 노이즈\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:  #(3) tf.GradientTape() 오픈\n",
    "        generated_images = generator(noise, training=True)  #(4) generated_images 생성\n",
    "\n",
    "        #(5) discriminator 판별\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        #(6) loss 계산\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        #(7) accuracy 계산\n",
    "        real_accuracy, fake_accuracy = discriminator_accuracy(real_output, fake_output) \n",
    "    \n",
    "    #(8) gradient 계산\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    #(9) 모델 학습\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss, real_accuracy, fake_accuracy  #(10) 리턴값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "referenced-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, it, sample_seeds):\n",
    "\n",
    "    predictions = model(sample_seeds, training=False)\n",
    "    predictions = (predictions + 1) / 2\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        plt.imshow(predictions[i])\n",
    "        plt.axis('off')\n",
    "   \n",
    "    plt.savefig(f'dcgan_newimage/cifar10/generated_samples/sample_epoch_{epoch}_iter_{round(it, 4)}.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hollow-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 6    # matlab 차트의 기본 크기를 15,6으로 지정해 줍니다.\n",
    "\n",
    "def draw_train_history(history, epoch):\n",
    "    # summarize history for loss  \n",
    "    plt.subplot(211)  \n",
    "    plt.plot(history['gen_loss'])  \n",
    "    plt.plot(history['disc_loss'])  \n",
    "    plt.title('model loss')  \n",
    "    plt.ylabel('loss')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['gen_loss', 'disc_loss'], loc='upper left')  \n",
    "\n",
    "    # summarize history for accuracy  \n",
    "    plt.subplot(212)  \n",
    "    plt.plot(history['fake_accuracy'])  \n",
    "    plt.plot(history['real_accuracy'])  \n",
    "    plt.title('discriminator accuracy')  \n",
    "    plt.ylabel('accuracy')  \n",
    "    plt.xlabel('batch iters')  \n",
    "    plt.legend(['fake_accuracy', 'real_accuracy'], loc='upper left')  \n",
    "    \n",
    "    # training_history 디렉토리에 epoch별로 그래프를 이미지 파일로 저장합니다.\n",
    "    plt.savefig(f'dcgan_newimage/cifar10/training_history/train_history_{epoch}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "brilliant-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "adjustable-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'dcgan_newimage/cifar10/training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "developmental-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, save_every):\n",
    "    start = time.time()\n",
    "    history = {'gen_loss':[], 'disc_loss':[], 'real_accuracy':[], 'fake_accuracy':[]}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        for it, image_batch in enumerate(dataset):\n",
    "            gen_loss, disc_loss, real_accuracy, fake_accuracy = train_step(image_batch)\n",
    "            history['gen_loss'].append(gen_loss)\n",
    "            history['disc_loss'].append(disc_loss)\n",
    "            history['real_accuracy'].append(real_accuracy)\n",
    "            history['fake_accuracy'].append(fake_accuracy)\n",
    "\n",
    "            if it % 50 == 0:\n",
    "                display.clear_output(wait=True)\n",
    "                generate_and_save_images(generator, epoch+1, it+1, seed)\n",
    "                print('Epoch {} | iter {}'.format(epoch+1, it+1))\n",
    "                print('Time for epoch {} : {} sec'.format(epoch+1, int(time.time()-epoch_start)))\n",
    "\n",
    "        if (epoch + 1) % save_every == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        generate_and_save_images(generator, epochs, it, seed)\n",
    "        print('Time for training : {} sec'.format(int(time.time()-start)))\n",
    "\n",
    "        draw_train_history(history, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "purple-genius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_every = 5\n",
    "EPOCHS = 500\n",
    "\n",
    "# 사용가능한 GPU 디바이스 확인\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "charged-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "agreed-happiness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train(train_dataset, EPOCHS, save_every)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-yellow",
   "metadata": {},
   "source": [
    "## Loss, Metric 변화\n",
    "<img src=https://user-images.githubusercontent.com/43724189/140881564-ed4dbe78-b39d-449e-88ab-8edcde5f103c.png><img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "medium-hybrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan_newimage/cifar10/cifar10.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "    filenames = [i[1] for i in sorted([(int(i.split('_')[2]) * 1000 + int(i.split('_')[4].split('.')[0]), i) for i in os.listdir('dcgan_newimage/cifar10/generated_samples')])]\n",
    "    last = -1\n",
    "    for i, filename in enumerate(filenames):\n",
    "        filename = 'dcgan_newimage/cifar10/generated_samples/' + filename\n",
    "        frame = 2*(i**0.5)\n",
    "        if round(frame) > round(last):\n",
    "            last = frame\n",
    "        else:\n",
    "            continue\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-brush",
   "metadata": {},
   "source": [
    "## epoch별 이미지 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-allocation",
   "metadata": {},
   "source": [
    "<img src=https://user-images.githubusercontent.com/43724189/140881155-27f75034-9f05-441c-ad84-22eff25dbdc4.gif></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-lodging",
   "metadata": {},
   "source": [
    "## 마지막 epoch 생성 이미지\n",
    "- 무언가 버스 같기도, 새 같기도 하다.\n",
    "<img src=https://user-images.githubusercontent.com/43724189/140881286-d3771984-e8ee-4ccb-8bbb-5655a33fecbb.png width=200></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-period",
   "metadata": {},
   "source": [
    "## 추가 GAN 모델 구조: LSGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "equivalent-experience",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1, d1=2.388, d2=2.227 g=2.971\n",
      ">101, d1=0.729, d2=3.251 g=1.568\n",
      ">201, d1=0.897, d2=1.040 g=0.909\n",
      ">301, d1=0.638, d2=0.560 g=1.015\n",
      ">401, d1=1.011, d2=0.458 g=0.867\n",
      ">501, d1=0.420, d2=0.614 g=0.737\n",
      ">601, d1=0.578, d2=0.373 g=0.827\n",
      ">701, d1=0.471, d2=0.558 g=0.603\n",
      "Saved Generated_plot_000781.png and model_000781.h5\n",
      ">801, d1=0.663, d2=0.394 g=0.727\n",
      ">901, d1=0.679, d2=0.393 g=0.515\n",
      ">1001, d1=0.659, d2=0.214 g=0.483\n",
      ">1101, d1=0.403, d2=0.422 g=0.774\n",
      ">1201, d1=0.161, d2=0.555 g=0.885\n",
      ">1301, d1=0.401, d2=0.364 g=0.472\n",
      ">1401, d1=0.308, d2=0.338 g=0.483\n",
      ">1501, d1=0.341, d2=0.309 g=0.479\n",
      "Saved Generated_plot_001562.png and model_001562.h5\n",
      ">1601, d1=0.583, d2=0.293 g=0.598\n",
      ">1701, d1=0.458, d2=0.200 g=0.375\n",
      ">1801, d1=0.346, d2=0.245 g=0.242\n",
      ">1901, d1=0.279, d2=0.455 g=0.639\n",
      ">2001, d1=0.238, d2=0.194 g=0.379\n",
      ">2101, d1=0.126, d2=0.465 g=0.708\n",
      ">2201, d1=0.255, d2=0.224 g=0.579\n",
      ">2301, d1=0.278, d2=0.434 g=0.396\n",
      "Saved Generated_plot_002343.png and model_002343.h5\n",
      ">2401, d1=0.201, d2=0.260 g=0.446\n",
      ">2501, d1=0.184, d2=0.394 g=0.541\n",
      ">2601, d1=0.292, d2=0.149 g=0.396\n",
      ">2701, d1=0.343, d2=0.453 g=0.282\n",
      ">2801, d1=0.197, d2=0.139 g=0.410\n",
      ">2901, d1=0.349, d2=0.327 g=0.373\n",
      ">3001, d1=0.218, d2=0.288 g=0.345\n",
      ">3101, d1=0.315, d2=0.328 g=0.361\n",
      "Saved Generated_plot_003124.png and model_003124.h5\n",
      ">3201, d1=0.182, d2=0.151 g=0.527\n",
      ">3301, d1=0.232, d2=0.207 g=0.602\n",
      ">3401, d1=0.290, d2=0.271 g=0.610\n",
      ">3501, d1=0.183, d2=0.221 g=0.540\n",
      ">3601, d1=0.350, d2=0.160 g=0.563\n",
      ">3701, d1=0.210, d2=0.165 g=0.458\n",
      ">3801, d1=0.210, d2=0.131 g=0.373\n",
      ">3901, d1=0.140, d2=0.182 g=0.858\n",
      "Saved Generated_plot_003905.png and model_003905.h5\n",
      ">4001, d1=0.258, d2=0.306 g=0.514\n",
      ">4101, d1=0.294, d2=0.268 g=0.456\n",
      ">4201, d1=0.339, d2=0.199 g=0.377\n",
      ">4301, d1=0.147, d2=0.238 g=0.426\n",
      ">4401, d1=0.197, d2=0.092 g=0.389\n",
      ">4501, d1=0.251, d2=0.218 g=0.636\n",
      ">4601, d1=0.389, d2=0.347 g=0.550\n",
      "Saved Generated_plot_004686.png and model_004686.h5\n",
      ">4701, d1=0.278, d2=0.317 g=0.371\n",
      ">4801, d1=0.313, d2=0.206 g=0.295\n",
      ">4901, d1=0.235, d2=0.308 g=0.400\n",
      ">5001, d1=0.267, d2=0.239 g=0.364\n",
      ">5101, d1=0.234, d2=0.092 g=0.389\n",
      ">5201, d1=0.324, d2=0.244 g=0.310\n",
      ">5301, d1=0.336, d2=0.302 g=0.438\n",
      ">5401, d1=0.237, d2=0.275 g=0.356\n",
      "Saved Generated_plot_005467.png and model_005467.h5\n",
      ">5501, d1=0.366, d2=0.250 g=0.432\n",
      ">5601, d1=0.334, d2=0.361 g=0.320\n",
      ">5701, d1=0.208, d2=0.293 g=0.395\n",
      ">5801, d1=0.237, d2=0.207 g=0.543\n",
      ">5901, d1=0.371, d2=0.155 g=0.361\n",
      ">6001, d1=0.257, d2=0.226 g=0.365\n",
      ">6101, d1=0.187, d2=0.347 g=0.360\n",
      ">6201, d1=0.273, d2=0.278 g=0.381\n",
      "Saved Generated_plot_006248.png and model_006248.h5\n",
      ">6301, d1=0.265, d2=0.183 g=0.437\n",
      ">6401, d1=0.194, d2=0.240 g=0.469\n",
      ">6501, d1=0.346, d2=0.364 g=0.421\n",
      ">6601, d1=0.257, d2=0.176 g=0.411\n",
      ">6701, d1=0.261, d2=0.231 g=0.389\n",
      ">6801, d1=0.180, d2=0.269 g=0.452\n",
      ">6901, d1=0.320, d2=0.348 g=0.343\n",
      ">7001, d1=0.347, d2=0.295 g=0.402\n",
      "Saved Generated_plot_007029.png and model_007029.h5\n",
      ">7101, d1=0.236, d2=0.305 g=0.451\n",
      ">7201, d1=0.207, d2=0.215 g=0.580\n",
      ">7301, d1=0.272, d2=0.244 g=0.422\n",
      ">7401, d1=0.198, d2=0.165 g=0.363\n",
      ">7501, d1=0.199, d2=0.289 g=0.467\n",
      ">7601, d1=0.254, d2=0.267 g=0.385\n",
      ">7701, d1=0.295, d2=0.291 g=0.434\n",
      ">7801, d1=0.301, d2=0.245 g=0.392\n",
      "Saved Generated_plot_007810.png and model_007810.h5\n",
      ">7901, d1=0.332, d2=0.217 g=0.325\n",
      ">8001, d1=0.392, d2=0.314 g=0.371\n",
      ">8101, d1=0.359, d2=0.335 g=0.306\n",
      ">8201, d1=0.284, d2=0.354 g=0.343\n",
      ">8301, d1=0.316, d2=0.348 g=0.323\n",
      ">8401, d1=0.228, d2=0.282 g=0.382\n",
      ">8501, d1=0.368, d2=0.264 g=0.288\n",
      "Saved Generated_plot_008591.png and model_008591.h5\n",
      ">8601, d1=0.253, d2=0.289 g=0.381\n",
      ">8701, d1=0.280, d2=0.246 g=0.302\n",
      ">8801, d1=0.262, d2=0.202 g=0.391\n",
      ">8901, d1=0.311, d2=0.324 g=0.303\n",
      ">9001, d1=0.272, d2=0.294 g=0.304\n",
      ">9101, d1=0.322, d2=0.300 g=0.305\n",
      ">9201, d1=0.277, d2=0.306 g=0.319\n",
      ">9301, d1=0.335, d2=0.350 g=0.272\n",
      "Saved Generated_plot_009372.png and model_009372.h5\n",
      ">9401, d1=0.255, d2=0.335 g=0.408\n",
      ">9501, d1=0.328, d2=0.329 g=0.407\n",
      ">9601, d1=0.339, d2=0.359 g=0.420\n",
      ">9701, d1=0.303, d2=0.267 g=0.305\n",
      ">9801, d1=0.400, d2=0.224 g=0.306\n",
      ">9901, d1=0.271, d2=0.363 g=0.307\n",
      ">10001, d1=0.264, d2=0.260 g=0.296\n",
      ">10101, d1=0.285, d2=0.243 g=0.337\n",
      "Saved Generated_plot_010153.png and model_010153.h5\n",
      ">10201, d1=0.310, d2=0.367 g=0.329\n",
      ">10301, d1=0.303, d2=0.318 g=0.371\n",
      ">10401, d1=0.441, d2=0.315 g=0.285\n",
      ">10501, d1=0.265, d2=0.248 g=0.340\n",
      ">10601, d1=0.241, d2=0.331 g=0.317\n",
      ">10701, d1=0.341, d2=0.265 g=0.296\n",
      ">10801, d1=0.258, d2=0.312 g=0.373\n",
      ">10901, d1=0.311, d2=0.297 g=0.321\n",
      "Saved Generated_plot_010934.png and model_010934.h5\n",
      ">11001, d1=0.349, d2=0.322 g=0.322\n",
      ">11101, d1=0.328, d2=0.319 g=0.321\n",
      ">11201, d1=0.275, d2=0.265 g=0.317\n",
      ">11301, d1=0.289, d2=0.266 g=0.340\n",
      ">11401, d1=0.323, d2=0.318 g=0.309\n",
      ">11501, d1=0.326, d2=0.290 g=0.360\n",
      ">11601, d1=0.253, d2=0.328 g=0.302\n",
      ">11701, d1=0.293, d2=0.302 g=0.288\n",
      "Saved Generated_plot_011715.png and model_011715.h5\n",
      ">11801, d1=0.273, d2=0.269 g=0.369\n",
      ">11901, d1=0.289, d2=0.290 g=0.330\n",
      ">12001, d1=0.268, d2=0.285 g=0.338\n",
      ">12101, d1=0.298, d2=0.285 g=0.349\n",
      ">12201, d1=0.237, d2=0.275 g=0.255\n",
      ">12301, d1=0.312, d2=0.338 g=0.277\n",
      ">12401, d1=0.368, d2=0.301 g=0.308\n",
      "Saved Generated_plot_012496.png and model_012496.h5\n",
      ">12501, d1=0.296, d2=0.268 g=0.306\n",
      ">12601, d1=0.376, d2=0.324 g=0.288\n",
      ">12701, d1=0.330, d2=0.329 g=0.303\n",
      ">12801, d1=0.286, d2=0.326 g=0.309\n",
      ">12901, d1=0.356, d2=0.308 g=0.291\n",
      ">13001, d1=0.291, d2=0.298 g=0.337\n",
      ">13101, d1=0.301, d2=0.281 g=0.325\n",
      ">13201, d1=0.303, d2=0.293 g=0.329\n",
      "Saved Generated_plot_013277.png and model_013277.h5\n",
      ">13301, d1=0.278, d2=0.257 g=0.312\n",
      ">13401, d1=0.307, d2=0.302 g=0.314\n",
      ">13501, d1=0.289, d2=0.307 g=0.287\n",
      ">13601, d1=0.277, d2=0.315 g=0.300\n",
      ">13701, d1=0.308, d2=0.297 g=0.305\n",
      ">13801, d1=0.283, d2=0.316 g=0.323\n",
      ">13901, d1=0.259, d2=0.297 g=0.315\n",
      ">14001, d1=0.291, d2=0.305 g=0.304\n",
      "Saved Generated_plot_014058.png and model_014058.h5\n",
      ">14101, d1=0.292, d2=0.286 g=0.311\n",
      ">14201, d1=0.318, d2=0.318 g=0.307\n",
      ">14301, d1=0.325, d2=0.314 g=0.294\n",
      ">14401, d1=0.283, d2=0.297 g=0.285\n",
      ">14501, d1=0.323, d2=0.313 g=0.291\n",
      ">14601, d1=0.277, d2=0.285 g=0.297\n",
      ">14701, d1=0.297, d2=0.320 g=0.302\n",
      ">14801, d1=0.276, d2=0.286 g=0.292\n",
      "Saved Generated_plot_014839.png and model_014839.h5\n",
      ">14901, d1=0.313, d2=0.307 g=0.323\n",
      ">15001, d1=0.275, d2=0.304 g=0.306\n",
      ">15101, d1=0.335, d2=0.307 g=0.292\n",
      ">15201, d1=0.277, d2=0.299 g=0.317\n",
      ">15301, d1=0.304, d2=0.325 g=0.296\n",
      ">15401, d1=0.295, d2=0.263 g=0.314\n",
      ">15501, d1=0.302, d2=0.285 g=0.306\n",
      ">15601, d1=0.317, d2=0.334 g=0.299\n",
      "Saved Generated_plot_015620.png and model_015620.h5\n",
      "Saved plot_line_plot_loss.png\n"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape, Flatten, Conv2D, Dropout, Conv2DTranspose\n",
    "from keras.layers import Activation\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    " \n",
    "\n",
    "def define_discriminator(in_shape=(32,32,3)):\n",
    "\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (5,5), strides=(2,2), padding='same', kernel_initializer=init, input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (5,5), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='linear', kernel_initializer=init))\n",
    "\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    return model\n",
    " \n",
    "\n",
    "def define_generator(latent_dim):\n",
    "\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    n_nodes = 256 * 8 * 8\n",
    "    model.add(Dense(n_nodes, kernel_initializer=init, input_shape=(latent_dim,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Reshape((8, 8, 256)))\n",
    "    model.add(Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', kernel_initializer=init, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    model.add(Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', kernel_initializer=init, use_bias=False))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    " \n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "\n",
    "    for layer in discriminator.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "    return model\n",
    " \n",
    "\n",
    "def load_real_samples():\n",
    "\n",
    "    (X, _), (_, _) = cifar10.load_data()\n",
    "    X = X.astype('float32')\n",
    "    X = (X - 127.5) / 127.5\n",
    "    return X\n",
    " \n",
    "\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    X = dataset[ix]\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input\n",
    "\n",
    "\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    X = generator.predict(x_input)\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    " \n",
    "\n",
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\n",
    "    X = (X + 1) / 2.0\n",
    "\n",
    "    for i in range(10 * 10):\n",
    "        pyplot.subplot(10, 10, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X[i, :, :])\n",
    "    \n",
    "    filename1 = 'Generated_plot_%06d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    filename2 = 'model_%06d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    print('Saved %s and %s' % (filename1, filename2))\n",
    "\n",
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    pyplot.plot(d1_hist, label='dloss1')\n",
    "    pyplot.plot(d2_hist, label='dloss2')\n",
    "    pyplot.plot(g_hist, label='gloss')\n",
    "    pyplot.legend()\n",
    "    filename = 'plot_line_plot_loss.png'\n",
    "    pyplot.savefig(filename)\n",
    "    pyplot.close()\n",
    "    print('Saved %s' % (filename))\n",
    "\n",
    "    \n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=20, n_batch=64):\n",
    "\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "\n",
    "    half_batch = int(n_batch / 2)\n",
    "\n",
    "    d1_hist, d2_hist, g_hist = list(), list(), list()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "\n",
    "        X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "\n",
    "        d_loss1 = d_model.train_on_batch(X_real, y_real)\n",
    "        d_loss2 = d_model.train_on_batch(X_fake, y_fake)\n",
    "\n",
    "        z_input = generate_latent_points(latent_dim, n_batch)\n",
    "        y_real2 = ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(z_input, y_real2)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "\n",
    "        d1_hist.append(d_loss1)\n",
    "        d2_hist.append(d_loss2)\n",
    "        g_hist.append(g_loss)\n",
    "\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "            summarize_performance(i, g_model, latent_dim)\n",
    "\n",
    "    plot_history(d1_hist, d2_hist, g_hist)\n",
    "\n",
    "\n",
    "latent_dim = 100\n",
    "discriminator = define_discriminator()\n",
    "generator = define_generator(latent_dim)\n",
    "gan_model = define_gan(generator, discriminator)\n",
    "dataset = load_real_samples()\n",
    "\n",
    "train(generator, discriminator, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-soccer",
   "metadata": {},
   "source": [
    "<img src=https://user-images.githubusercontent.com/43724189/142004701-b025f588-d133-4bf4-9dbe-10511b5b0b5d.png></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
