{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "communist-antigua",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-counter",
   "metadata": {},
   "source": [
    "DeepLab V3+\n",
    "- https://blog.lunit.io/2018/07/02/deeplab-v3-encoder-decoder-with-atrous-separable-convolution-for-semantic-image-segmentation/\n",
    "- https://github.com/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "narrow-coordinator",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "\n",
    "model_dir = 'human_segmentation/models'\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "model_path = os.path.join(model_dir, 'deeplab_model.tar.gz')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    urllib.request.urlretrieve(DOWNLOAD_URL_PREFIX + 'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz', model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coated-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.compat.v1.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    def preprocess(self, img_orig):\n",
    "        height, width = img_orig.shape[:2]\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = cv2.resize(img_orig, target_size)\n",
    "        resized_rgb = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
    "        img_input = resized_rgb\n",
    "        return img_input\n",
    "        \n",
    "    def run(self, image):\n",
    "        img_input = self.preprocess(image)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [img_input]})\n",
    "\n",
    "        seg_map = batch_seg_map[0]\n",
    "        return cv2.cvtColor(img_input, cv2.COLOR_RGB2BGR), seg_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-voluntary",
   "metadata": {},
   "source": [
    "# 1. 인물모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chronic-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFocus:\n",
    "    def __init__(self, model_path, img_path, label):\n",
    "        self.model = DeepLabModel(model_path)\n",
    "        self.img = cv2.imread(img_path)\n",
    "        self.resize_img, self.segmentation_map = self.model.run(self.img)\n",
    "        self.segmentation_map = self.extract_label(self.segmentation_map, label)\n",
    "    \n",
    "    def extract_label(self, segmentation_map, label):\n",
    "        labels = [\n",
    "            'background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', \n",
    "            'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tv'\n",
    "        ]\n",
    "        idx = labels.index(label)\n",
    "        segmentation_map = np.where(segmentation_map==idx, idx, 0)\n",
    "        return segmentation_map\n",
    "    \n",
    "    def make_shallow(self, show_origin=True, show_mask=False):\n",
    "        if show_origin:\n",
    "            plt.imshow(cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "        \n",
    "        img_mask = self.segmentation_map * (255/self.segmentation_map.max())\n",
    "        img_mask = img_mask.astype(np.uint8)\n",
    "        img_mask_up = cv2.resize(img_mask, self.img.shape[:2][::-1], interpolation=cv2.INTER_LINEAR)\n",
    "        _, img_mask_up = cv2.threshold(img_mask_up, 128, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "        img_orig_blur = cv2.blur(self.img, (20,20))\n",
    "        img_mask_color = cv2.cvtColor(img_mask_up, cv2.COLOR_GRAY2BGR)\n",
    "        img_bg_mask = cv2.bitwise_not(img_mask_color)\n",
    "        img_bg_blur = cv2.bitwise_and(img_orig_blur, img_bg_mask)\n",
    "        \n",
    "        if show_mask:\n",
    "            plt.imshow(cv2.cvtColor(img_bg_blur, cv2.COLOR_BGR2RGB))\n",
    "            plt.show()\n",
    "        \n",
    "        img_concat = np.where(img_mask_color==255, self.img, img_bg_blur)\n",
    "        plt.imshow(cv2.cvtColor(img_concat, cv2.COLOR_BGR2RGB))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "respiratory-marketing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/my_image.jpg'\n",
    "label = 'person'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-revision",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/3NFJX0M/2021-10-23-11-31-10.png width=300></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enhanced-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/bus.jpg'\n",
    "label = 'bus'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-compatibility",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/RpNfS2K/2021-10-23-11-31-19.png width=300></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accepted-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/dogs.jpg'\n",
    "label = 'dog'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-scheduling",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/ryLps73/2021-10-23-11-31-25.png width=300></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pharmaceutical-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/dog.jpg'\n",
    "label = 'dog'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-chance",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/RcqNZSk/2021-10-23-11-31-32.png width=300></img>\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raising-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/bicycle person.jpg'\n",
    "label = 'person'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-master",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/gv7BGh2/2021-10-23-11-31-38.png width=300></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "synthetic-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'human_segmentation/images/bicycle person.jpg'\n",
    "label = 'bicycle'\n",
    "app = ShallowFocus(model_path, img_path, label)\n",
    "app.make_shallow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-rover",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/KFjWgSL/2021-10-23-11-31-42.png width=300></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-bidder",
   "metadata": {},
   "source": [
    "# 2. 사진에서 문제점 찾기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-mercury",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/4tKYg1q/2021-10-23-12-40-09.png></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-bennett",
   "metadata": {},
   "source": [
    "- 첫번째 사진의 경우, 사람을 segmentation하는 사진인데, 발 부분을 탐지하지 못했습니다.\n",
    "- 두번째 사진은 개를 탐지해야 하는데, 귀 부분을 탐지하지 못했습니다.\n",
    "- 세번째 사진은 버스를 탐지해야 하는데, 버스 뒷쪽 영역을 탐지하지 못했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-multimedia",
   "metadata": {},
   "source": [
    "# 3. 해결 방법 제안"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-capability",
   "metadata": {},
   "source": [
    "## FuseNet: Incorporating Depth into Semantic Segmentation via Fusion-based CNN Architecture\n",
    "Caner Hazirbas, Lingni Ma, Csaba Domokos, and Daniel Cremers / 382회 인용\n",
    "\n",
    "\n",
    "https://vision.in.tum.de/_media/spezial/bib/hazirbasma2016fusenet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-november",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "<img src=https://i.ibb.co/mzXFvBf/2021-10-26-00-30-28.png></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-norman",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/CHTjRry/2021-10-26-00-42-33.pngm></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-context",
   "metadata": {},
   "source": [
    "모델은 인코더, 디코더로 구성되어 있으며, 2개의 이미지(RGB image, Depth image)를 input으로 받는 모델입니다.\n",
    "각 이미지별로 Convolution, Batch Normalization, ReLU 블록(CBR 블록)을 2번 거친 후, 요소합을 수행합니다.\n",
    "다만 2가지 전략을 실험해 봤는데, \n",
    "1. 앞서 말했듯이 위 CBR 블록을 2번 거친 후 요소 합을 수행하는 방법 (a. Sparse fusion(SF))\n",
    "2. CBR 블록을 1번 거친 후, 요소 합을 하고, CBR을 다시 1번 거친 후 요소 합을 하는 방법으로 (b. Dense fusion (DF)) \n",
    "\n",
    "실험을 진행했다고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-improvement",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/fYbDzSz/2021-10-26-00-52-53.png></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-sapphire",
   "metadata": {},
   "source": [
    "테스트 결과, Dense fusion보다는 Sparse fusion이 성능이 더 좋았습니다.  \n",
    "그리고 Spare fusion의 경우, fusion layer 많아질수록 성능이 높아지는 경향을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-blood",
   "metadata": {},
   "source": [
    "위 FuseNet과 앞선 DeepLab V3+의 아이디어(Atrous convolution)를 결합하면 효과가 높아질 수 있을것이라 생각했고,  \n",
    "이러한 아이디어는 아래 논문에서 확인할 수 있었습니다.\n",
    "\n",
    "## Improved Multi-modal Network Using Dilated Convolution Pyramid Pooling \n",
    "박준영, 호요성 \n",
    "- https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201815540967076&oCn=NPAP12901063&dbt=CFKO&journal=NPRO00379579  \n",
    "  \n",
    "NYUDv2 데이터셋을 사용한 실험 결과는 아래와 같고, IoU가 1.8%p 상승함을 확인할 수 있었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-genesis",
   "metadata": {},
   "source": [
    "<img src=https://i.ibb.co/QfRy0Lm/2021-10-26-10-44-42.png width=400></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
